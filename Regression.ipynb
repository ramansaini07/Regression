{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_24bqFe-oeEK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ** Regression:**\n",
        "\n",
        "**1. What is Simple Linear Regression?**  \n",
        "Simple Linear Regression models the relationship between **two variables** — one independent (X) and one dependent (Y) — by fitting a straight line: **Y = mX + c**.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**2. What are the key assumptions of Simple Linear Regression?**  \n",
        "- Linearity: Relationship between X and Y is linear.  \n",
        "- Independence: Observations are independent.  \n",
        "- Homoscedasticity: Constant variance of residuals.  \n",
        "- Normality: Residuals are normally distributed.\n",
        "\n",
        "---\n",
        "\n",
        "**3. What does the coefficient m represent in the equation Y = mX + c?**  \n",
        "The **slope (m)** represents the **change in Y** for a **one-unit change in X**.\n",
        "\n",
        "---\n",
        "\n",
        "**4. What does the intercept c represent in the equation Y = mX + c?**  \n",
        "The **intercept (c)** is the **predicted value of Y when X = 0**.\n",
        "\n",
        "---\n",
        "\n",
        "**5. How do we calculate the slope m in Simple Linear Regression?**  \n",
        "\\[\n",
        "m = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum (X_i - \\bar{X})^2}\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "**6. What is the purpose of the least squares method in Simple Linear Regression?**  \n",
        "It finds the line that **minimizes the sum of squared differences** between observed and predicted Y values (minimizes errors).\n",
        "\n",
        "---\n",
        "\n",
        "**7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**  \n",
        "R² measures the **percentage of the variance in Y** that is **explained by X**.  \n",
        "- R² = 1 → perfect fit  \n",
        "- R² = 0 → no relationship\n",
        "\n",
        "---\n",
        "\n",
        "### **Multiple Linear Regression:**\n",
        "\n",
        "**8. What is Multiple Linear Regression?**  \n",
        "Regression where **multiple independent variables** predict a **single dependent variable**.\n",
        "\n",
        "---\n",
        "\n",
        "**9. What is the main difference between Simple and Multiple Linear Regression?**  \n",
        "- **Simple**: One predictor (X).  \n",
        "- **Multiple**: Two or more predictors (X₁, X₂, ... Xn).\n",
        "\n",
        "---\n",
        "\n",
        "**10. What are the key assumptions of Multiple Linear Regression?**  \n",
        "- Linearity  \n",
        "- Independence of errors  \n",
        "- Homoscedasticity  \n",
        "- Normality of errors  \n",
        "- No multicollinearity between predictors\n",
        "\n",
        "---\n",
        "\n",
        "**11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**  \n",
        "**Heteroscedasticity** means **non-constant variance** of residuals.  \n",
        "It leads to **biased standard errors** → **incorrect p-values** → **wrong conclusions**.\n",
        "\n",
        "---\n",
        "\n",
        "**12. How can you improve a Multiple Linear Regression model with high multicollinearity?**  \n",
        "- Remove highly correlated predictors  \n",
        "- Combine predictors  \n",
        "- Use Ridge or Lasso Regression  \n",
        "- Apply Principal Component Analysis (PCA)\n",
        "\n",
        "---\n",
        "\n",
        "**13. What are some common techniques for transforming categorical variables for use in regression models?**  \n",
        "- **One-hot encoding**  \n",
        "- **Label encoding**  \n",
        "- **Ordinal encoding** (for ordered categories)\n",
        "\n",
        "---\n",
        "\n",
        "**14. What is the role of interaction terms in Multiple Linear Regression?**  \n",
        "They **capture the combined effect** of two or more variables that is **different from their individual effects**.\n",
        "\n",
        "---\n",
        "\n",
        "**15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**  \n",
        "- **Simple**: Value of Y when X = 0.  \n",
        "- **Multiple**: Value of Y when **all predictors = 0** (may not always be meaningful).\n",
        "\n",
        "---\n",
        "\n",
        "**16. What is the significance of the slope in regression analysis, and how does it affect predictions?**  \n",
        "The slope tells us how much the dependent variable **changes for a one-unit change** in the independent variable, **holding other variables constant** (in multiple regression).\n",
        "\n",
        "---\n",
        "\n",
        "**17. How does the intercept in a regression model provide context for the relationship between variables?**  \n",
        "It sets the **baseline value** for Y when **all predictors are at their reference values (usually 0)**.\n",
        "\n",
        "---\n",
        "\n",
        "**18. What are the limitations of using R² as a sole measure of model performance?**  \n",
        "- R² **always increases** with more variables (even if they are useless).  \n",
        "- It doesn’t measure **causality** or **model quality**.  \n",
        "- Better to check **Adjusted R²**.\n",
        "\n",
        "---\n",
        "\n",
        "**19. How would you interpret a large standard error for a regression coefficient?**  \n",
        "It suggests the coefficient estimate is **unstable** and possibly **not statistically significant**.\n",
        "\n",
        "---\n",
        "\n",
        "**20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?**  \n",
        "- Identified by a **funnel shape** or **pattern** in residual vs fitted plots.  \n",
        "- Important because it **violates assumptions** and **biases standard errors**.\n",
        "\n",
        "---\n",
        "\n",
        "**21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**  \n",
        "It means **some predictors** are **not actually useful**, and **Adjusted R² penalizes** for those unnecessary variables.\n",
        "\n",
        "---\n",
        "\n",
        "**22. Why is it important to scale variables in Multiple Linear Regression?**  \n",
        "- To ensure that predictors with large ranges don’t **dominate the model**.  \n",
        "- Some algorithms assume **normalized input** (like Ridge or Lasso).\n",
        "\n",
        "---\n",
        "\n",
        "### **Polynomial Regression:**\n",
        "\n",
        "**23. What is polynomial regression?**  \n",
        "It models the relationship between X and Y as an **nth degree polynomial** (not just a straight line).\n",
        "\n",
        "---\n",
        "\n",
        "**24. How does polynomial regression differ from linear regression?**  \n",
        "- **Linear Regression** fits a **straight line**.  \n",
        "- **Polynomial Regression** fits a **curved line**.\n",
        "\n",
        "---\n",
        "\n",
        "**25. When is polynomial regression used?**  \n",
        "When the relationship between X and Y is **non-linear**, but can still be modeled with a **polynomial curve**.\n",
        "\n",
        "---\n",
        "\n",
        "**26. What is the general equation for polynomial regression?**  \n",
        "\\[\n",
        "Y = b_0 + b_1X + b_2X^2 + b_3X^3 + ... + b_nX^n\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "**27. Can polynomial regression be applied to multiple variables?**  \n",
        "Yes, it's called **multivariate polynomial regression**, and it includes terms like \\( X_1^2, X_1X_2 \\), etc.\n",
        "\n",
        "---\n",
        "\n",
        "**28. What are the limitations of polynomial regression?**  \n",
        "- Can easily **overfit** data.  \n",
        "- Higher degree polynomials become **unstable** outside the training range (extrapolation problem).\n",
        "\n",
        "---\n",
        "\n",
        "**29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?**  \n",
        "- Cross-validation  \n",
        "- Adjusted R²  \n",
        "- AIC (Akaike Information Criterion) / BIC (Bayesian Information Criterion)\n",
        "\n",
        "---\n",
        "\n",
        "**30. Why is visualization important in polynomial regression?**  \n",
        "To visually check if the **curve fits the data** well without **overfitting** or **underfitting**.\n",
        "\n",
        "---\n",
        "\n",
        "**31. How is polynomial regression implemented in Python?**  \n",
        "Example using **scikit-learn**:\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Example Data\n",
        "X = [[1], [2], [3], [4], [5]]\n",
        "y = [1, 4, 9, 16, 25]\n",
        "\n",
        "# Transforming to polynomial features\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Model training\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, y)\n",
        "\n",
        "# Prediction\n",
        "y_pred = model.predict(X_poly)\n",
        "```\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GORFI1kFpIES"
      }
    }
  ]
}